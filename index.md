Daniel Gitelman, dgitelman@ucsd.edu

Neural Network Compression with Error Gurantees, A15
Mentors: Alex Cloninger, Rayan Saab

1. The most interesting topic we have uncovered this quarter was neural tangent kernels, and how they can be used to implement lossless compression. 
2. A potential investigation we might want to explore in quarter 2 would be to extend the results of lossless compression with neural tangent kernels in fully connected networks to convolutional layers.
3. One potential change that would be made to our quarter one project would be to shift the focus of the compression method from quantization to use of neural tangent kernels, while still retaining the use of error bounds and mathematical proofs.
4. I'm interested in using more proofs to demonstrate why a technique will work, especially in regards to how they can be used to create objective error bounds.
